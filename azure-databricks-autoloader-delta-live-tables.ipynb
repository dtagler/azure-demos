{"cells":[{"cell_type":"markdown","source":["# Azure Databricks Tutorial"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"212d7736-8b31-4047-86dc-1abdc95771c7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# databricks magic commands:\n# %sh\n# %fs\n# %python\n# %sql\n# %pip install\n# %conda\n# %matplotlib"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9f3b27f-f4f5-4bb8-b4d6-abfe44cd654f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Azure Service Principal Auth Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19e7732c-f83f-4493-9b2e-53051652064b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# note: all sensitive data should be stored in a key vault / databricks secret scope\n# azure portal: https://<Databricks_url>#secrets/createScope\n# use dbutils.secrets.get(scope = \"\", key = \"\")\n# databricks cli secret option:\n# databricks secrets list --scope <scope-name>\n# databricks secrets put --scope <scope-name> --key <key-name>\n# function for dbutils.secrets.get:\n# def get_keys(keyName:str) -> str:\n#     return dbutils.secrets.get(scope = 'formula1-scope', key = keyName)\n\n# azure subscription - resource providers - register eventgrid\nsubscription_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n\n# azure active directory - app registration portal\napplication_client_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\napplication_client_secret = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\ndirectory_tenant_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n\n# azure storage account portal\n# give app registration \"blob contrib\" role on storage account\nstorage_account = \"...\"\nstorage_container = \"...\"\nstorage_path = \"\"\n\n# azure storage endpoint options:\n# HTTPS: https://storageaccount.blob.core.windows.net/container/path/to/blob\n# WASBS - Windows Azure Storage Blob Secure: wasbs://containername@accountname.blob.core.windows.net\n# ABFSS - Azure blob file system secure: abfss://filesystemname@accountname.dfs.core.windows.net\n# note: you can mount this location in /mnt/ via dbutils.fs.mount()\nstorage_endpoint_url = (\n    f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net/{storage_path}\"\n)\nif not storage_endpoint_url.endswith(\"/\"):\n    storage_endpoint_url += \"/\"\n\n# azure databricks auth setup\nspark.conf.set(\n    f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\"\n)\nspark.conf.set(\n    f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n)\nspark.conf.set(\n    f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\",\n    application_client_id,\n)\nspark.conf.set(\n    f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\",\n    application_client_secret,\n)\nspark.conf.set(\n    f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n    f\"https://login.microsoftonline.com/{directory_tenant_id}/oauth2/token\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f2c850e-4662-42c9-88dd-66eac5f42fcf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Interacting with File System"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcbe8064-932d-4b6e-b900-a5f64034213c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# file system utils\ndbutils.fs.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25bc729b-7936-44bf-9257-6d490c8ad5ac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# databricks dbfs root\ndbutils.fs.ls(\".\")\n\n# external/mounted storage\n# dbutils.fs.ls(\"/mnt\")\n# dbutils.fs.mount() / dbutils.fs.unmount() / dbutils.fs.refreshMounts()\ndbutils.fs.mounts()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7c22ff9-4cfe-4cd2-b15b-540ede735607","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# list dir\nimport pandas as pd\npd.DataFrame(dbutils.fs.ls(storage_endpoint_url))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ed7445c-7220-4ca6-aa82-b20e32ee1f70","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe4dc397-2139-4f0e-b090-8cd659b71d94","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# custom schema and data types\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    TimestampType,\n    StringType,\n    DoubleType,\n)\ncustomschema = StructType(\n    [\n        StructField(\"timestamp\", TimestampType(), True),\n        StructField(\"column_2\", DoubleType(), True),\n        StructField(\"column_3\", DoubleType(), True),\n        StructField(\"column_4\", DoubleType(), True),\n        StructField(\"column_5\", DoubleType(), True),\n    ]\n)\n\n# read csv example\n# read json example: df = spark.read.json(json_file_path)\n# read parquet example: df = spark.read.parquet(parquet_file_path)\nfilename = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX.csv\"\ndf = spark.read.csv(\n    f\"{storage_endpoint_url}/{filename}\", header=True, schema=customschema\n)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3161c107-36b0-4110-b5ec-9ebe1f0fa43e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Basic PySpark Operations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5f89660-d8e6-479e-bc42-f599a1f6a49e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# schema\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39b7a830-24cb-4608-a2ed-f330e701b157","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# preview data\ndf.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f77a1e99-2c08-475b-a8a4-9f15d99abca0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# basic stats\ndf.select(\"column_2\").describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e24a17e-034e-4092-a7fb-6b1b95ce8f24","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# other common spark commands\n\n# write csv example\n# df.write.format(\"csv\").option(\"header\",\"true\").save(\"hdfs:///.../\")\n\n# write file to disk in parquet format - overwrite\n# df.write.partitionBy('').format('parquet').mode('overwrite').save()\n# write file to disk in parquet format - append\n# df.write.partitionBy('').format('parquet').mode('append').save()\n\n# from pyspark.sql.functions import lit <- constant columns\n\n# EDA/ETL functions:\n# spark.createDataFrame()\n# .columns / .dtypes\n# .select()\n# .withColumn() <- add columns\n# .cast() <- change type\n# .alias()\n# .withColumnRenamed() <- rename column\n# .drop() <- remove column\n# .where() / .filter()\n# .join() / .union() / .merge()\n# .pivot() / .melt()\n# .groupBy().avg()/.sum()/.agg() <- group by, aggregate, and apply functions\n# .sort()\n# .count()\n# .dropna() / .fillna()\n# .limit()\n# .take()\n# .map() / .foreach()\n# .explode()\n# .show()\n# .cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"618e89a3-adfb-4459-a219-5d0ee9c3d5c1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Pandas Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f0c06c9-b5c8-4bec-a828-a241cfa7b8af","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# alternative to standard PySpark SQL api\nimport pandas as pd\nimport pyspark.pandas as ps\n\n# spark dataframe to pandas-on-spark dataframe\ndf_ps = df.to_pandas_on_spark()\n\n# pandas to pandas-on-spark\ndf_pandas = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [5, 6, 7]})\ndf_pyspark_pandas = ps.from_pandas(df_pandas)\n\n# pandas-on-spark to pandas\ndf_pandas = df_pyspark_pandas.to_pandas()\n\n# pandas-on-spark to spark dataframe\ndf_spark = df_pyspark_pandas.to_spark()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70c62ea6-f41f-4601-a441-7e8f9bb12e26","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# preview\ndf_ps"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9dcc93e-cc0f-4645-8734-73cd18b48cfd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# dataframe info\ndf_ps.info()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97210ac7-f1e7-4686-b3cb-4c5482310712","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# basic operations\nmy_column = df_ps[\"column_2\"]\nmy_sum = df_ps[\"column_2\"].sum()\nmy_min = df_ps[\"column_3\"].min()\n\n# new column\ndf_ps[\"new_column\"] = df_ps[\"column_2\"] + df_ps[\"column_3\"]\n\n# filter\ndf_ps_filtered = df_ps[df_ps[\"new_column\"] > 1]\n\n# apply\ndef add_one(val):\n    return val + 1\n\n\ndf_ps_filtered_and_add_one = df_ps_filtered[[\"column_2\", \"column_3\", \"column_4\"]].apply(\n    add_one, axis=0\n)\ndf_ps_filtered_and_add_one"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a10e03a-faa5-44a0-93ec-e4905f056124","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Autoloader (Streaming Data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e8bac88-3e22-4c82-b14f-db72625dae9b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Azure Service Principal Auth Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bf17364-a33d-4667-b3b2-a0e2b1d96edf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# azure storage account portal\n# resource group\nstorage_resource_group = \"...\"\n# access key\nstorage_sas_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n# generate queue connection string\nstorage_queue_connection_string = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\n# databricks autoloader config\n# uses schema infer and evolution (rescues new columns)\nspark.conf.set(\"spark.databricks.cloudfiles.schemaInference.sampleSize.numFiles\", 10)\nspark.conf.set(\n    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", storage_sas_key\n)\ncheckpoint_location = storage_endpoint_url + \"_checkpoint/\"\ncloudfile = {\n    \"cloudFiles.subscriptionID\": subscription_id,\n    \"cloudFiles.connectionString\": storage_queue_connection_string,\n    \"cloudFiles.format\": \"csv\",\n    \"cloudFiles.tenantId\": directory_tenant_id,\n    \"cloudFiles.clientId\": application_client_id,\n    \"cloudFiles.clientSecret\": application_client_secret,\n    \"cloudFiles.resourceGroup\": storage_resource_group,\n    \"cloudFiles.inferColumnTypes\": \"true\",\n    # location for schema and checkpoint data\n    \"cloudFiles.schemaLocation\": checkpoint_location,\n    \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n    \"cloudFiles.useNotifications\": \"true\",\n}\nadditional_options = {\"header\": True, \"rescueDataColumn\": \"_rescued_data\"}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6d39501-2fb3-4662-ab0b-9b78d14ae42d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read and Write Streaming Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88e61cdb-7cfe-47b3-8e8f-147429163238","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# read streaming files from azure data lake\ndf_autoloader = (\n    spark.readStream.format(\"cloudFiles\")\n    # input options\n    .options(**cloudfile).options(**additional_options)\n    # define custom schema -> .schema()\n    # input location\n    .load(storage_endpoint_url)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d04af79-5512-45da-b6fc-e8d5b2b8c26a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# write delta table to azure data lake\ndf_autoloader_stream = (\n    df_autoloader.writeStream.format(\"delta\")\n    .outputMode(\"append\")\n    .queryName(\"example_query_name\")\n    # run function on each batche / merge with existing data use .foreachBatch()\n    .option(\"checkpointLocation\", checkpoint_location)\n    # trigger options: processingTime, availableNow, once\n    .trigger(once=True)\n    # output location\n    .start(storage_endpoint_url + \"data_table_output\")\n    # use .table() to save to databricks table\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fb4313a-9197-495e-8814-16b2ab250770","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# status\nprint(df_autoloader_stream.status)\nprint(df_autoloader_stream.recentProgress)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d31dedf-73b0-43d0-a103-7ba8fcc3dd9a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### EDA via PySpark API"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"994efde0-6768-4cb8-9894-297adee1c945","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# read delta table from azure data lake\ndf_load = spark.read.format(\"delta\").load(storage_endpoint_url + \"data_table_output\")\ndisplay(df_load)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b84ba3ae-1148-4b52-aefa-2334a0e24f62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# run pyspark query\n(\n    df_load.where(\n        \"column_2 > 0.9 AND column_3 > 0.9 AND column_4 > 0.9 AND column_5 > 0.9\"\n    )\n    .sort(\"timestamp\")\n    .display()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"203299d8-91c8-4943-bc36-0593b5a143d1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### EDA via SQL API"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02680285-6ea8-49fc-9c49-2565ad8b7982","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# save to databricks local table\ntable_name = \"my_test_table\"\ndf_load.write.saveAsTable(table_name, mode=\"overwrite\")\n\n# save variable name for use in %SQL statements\nspark.conf.set('personal.table_name', table_name)\n\n# delete table\n# spark.sql(f\"DROP TABLE {table_name}\")\n\n# other SQL commands\n# create table -> CREATE TABLE IF NOT EXISTS or CREATE OR REPLACE TABLE"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e32f8a3c-c5ec-4ff6-9bc0-fdf593a12392","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#  table information via pyspark sql api\ndisplay(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8a5d6e4-09a8-4fb4-abd8-339e91a7925f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql \nDESCRIBE DETAIL ${personal.table_name}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"d6e8726d-88a2-4946-99fa-2e013875357d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n/* run SQL query */\nSELECT\n  *\nFROM\n  ${personal.table_name}\nWHERE\n  column_2 > 0.9\n  AND column_3 > 0.9\n  AND column_4 > 0.9\n  AND column_5 > 0.9\nORDER BY\n  timestamp ASC\n  /* \n  other commands:\n  INSERT INTO table_name SELECT * FROM table_name_2\n  INSERT OVERWRITE TABLE table_name SELECT * FROM table_name_2\n  UPDATE \n  DELETE FROM\n  DESCRIBE \n  OPTIMIZE ... ZORDER BY\n  */"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"ed5a47c5-57bd-4266-821b-fb254df2bbd5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# save sql query results as spark dataframe\nnew_dataframe_name = _sqldf\ndisplay(new_dataframe_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"554e45b4-67b4-4f30-a17d-f510af0a0bec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Delta Live Tables Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5eca3312-d3da-481b-9e44-3e5eaa55ace0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# NOTE: this does not work in a notebook, you must create a DLT pipeline\n# GOTO: \"workflows\" - > \"delta live tables\" -> \"create pipeline\" -> select notebook\n\n# example syntax:\n# %pip install dlt\n# import dlt\n# @dlt.table(\n#   name=\"<name>\",\n#   comment=\"<comment>\",\n#   spark_conf={\"<key>\" : \"<value\", \"<key\" : \"<value>\"},\n#   table_properties={\"<key>\" : \"<value>\", \"<key>\" : \"<value>\"},\n#   path=\"<storage-location-path>\",\n#   partition_cols=[\"<partition-column>\", \"<partition-column>\"],\n#   schema=\"schema-definition\",\n#   temporary=False)\n# @dlt.expect\n# @dlt.expect_or_fail\n# @dlt.expect_or_drop\n# @dlt.expect_all\n# @dlt.expect_all_or_drop\n# @dlt.expect_all_or_fail\n# def <function-name>():\n#     return (<query>)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2475f976-aad1-400f-bb1c-ab78cb0eae62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"raw data, bronze\")\ndef dlt_table_bronze():\n    return (\n        spark.readStream.format(\"cloudFiles\")\n        .options(**cloudfile)\n        .options(**additional_options)\n        .load(storage_endpoint_url)\n    )\n\n@dlt.table(comment=\"data subset, silver\")\ndef dlt_table_silver():\n    return dlt.read(\"dlt_table_bronze\").where(\"column_2 > 0.9\")\n\n@dlt.table(comment=\"final data, gold\")\ndef dlt_table_gold():\n    return dlt.read(\"dlt_table_silver\").where(\"column_3 > 0.9\").sort(\"timestamp\").limit(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc494a52-d9fe-426c-ba00-9c2599f02e6a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## References"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"160fb3d2-1ad4-44ed-9e34-2b7818f8baa9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["- https://docs.databricks.com/ingestion/auto-loader/patterns.html\n- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-python-ref.html#python-spec\n- https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-data-sources.html\n- https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html\n- https://api-docs.databricks.com/python/pyspark/latest/pyspark.pandas/frame.html"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7656a50e-0ae9-4bce-bafd-8db9498dcd3e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcd28046-b0d5-4be3-b043-b4a3e1860dd9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"testing_export","dashboards":[{"elements":[{"elementNUID":"f8a5d6e4-09a8-4fb4-abd8-339e91a7925f","dashboardResultIndex":0,"guid":"873f862b-90c2-4716-a200-579172dbf67a","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":2,"width":21,"z":null},"elementType":"command"}],"guid":"d9f45fbd-0109-41a0-b51e-ed6085c775a9","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"09a897a7-9f96-4bfa-80f9-e761bb0c1338","origId":3710895382054801,"title":"TESTDASHBOARD","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2756703681155773,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3710895382054758}},"nbformat":4,"nbformat_minor":0}
